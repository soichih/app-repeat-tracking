#!/bin/bash

## file inputs
ANAT=`jq -r '.anat' config.json`

MASK=`jq -r '.mask' config.json`
PARC=`jq -r '.parc' config.json`
FA=`jq -r '.fa' config.json`

## CSD fits
LMAX2=`jq -r '.lmax2' config.json`
LMAX4=`jq -r '.lmax4' config.json`
LMAX6=`jq -r '.lmax6' config.json`
LMAX8=`jq -r '.lmax8' config.json`
LMAX10=`jq -r '.lmax10' config.json`
LMAX12=`jq -r '.lmax12' config.json`
LMAX14=`jq -r '.lmax14' config.json`

## list of files to move
TFILE=$ANAT
NFILE=$MASK,$PARC,$FA

if [ -f $LMAX2 ]; then
    #echo "Transferring lmax2..."
    TFILE=$TFILE,$LMAX2
fi

if [ -f $LMAX4 ]; then
    #echo "Transferring lmax4..."
    TFILE=$TFILE,$LMAX4
fi

if [ -f $LMAX6 ]; then
    #echo "Transferring lmax6..."
    TFILE=$TFILE,$LMAX6
fi

if [ -f $LMAX8 ]; then
    #echo "Transferring lmax8..."
    TFILE=$TFILE,$LMAX8
fi

if [ -f $LMAX10 ]; then
    #echo "Transferring lmax10..."
    TFILE=$TFILE,$LMAX10
fi

if [ -f $LMAX12 ]; then
    #echo "Transferring lmax12..."
    TFILE=$TFILE,$LMAX12
fi

if [ -f $LMAX14 ]; then
    #echo "Transferring lmax14..."
    TFILE=$TFILE,$LMAX14
fi

NFIB=`jq -r '.num_fibers' config.json`

## use what exists to build file list

project="" #project name should be set via "connect project" command on osgconnect
if [ $(hostname) == "csiu.grid.iu.edu" ]; then
	project="+ProjectName=\"brainlifeio\""
fi

cat > part1.submit << EOF
universe = vanilla
executable = submit_job.sh

$project
Requirements = HAS_SINGULARITY == TRUE

+SingularityImage = "/cvmfs/singularity.opensciencegrid.org/brainlife/mrtrix3:3.0_RC3"
+SingularityBindCVMFS = True

should_transfer_files = IF_NEEDED
when_to_transfer_output = ON_EXIT

# Send the job to Held state on failure.
on_exit_hold = (ExitBySignal == True) || (ExitCode != 0)

# Periodically retry the jobs every 10 minutes, up to a maximum of 5 retries.
periodic_release =  (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 600)

# remove job if it fails 5 times.
periodic_remove = (NumJobStarts == 5)

transfer_input_files = $TFILE
transfer_output_files = intermediate

#stream_output = True
output = part1.stdout.log
error = part1.stderr.log
log = part1.condor.out

queue
EOF

cat > part2.submit << EOF
universe = vanilla
executable = queue_jobs.sh

$project
Requirements = HAS_SINGULARITY == TRUE

+SingularityImage = "/cvmfs/singularity.opensciencegrid.org/brainlife/mrtrix3:3.0_RC3"
+SingularityBindCVMFS = True

should_transfer_files = IF_NEEDED
when_to_transfer_output = ON_EXIT

# Send the job to Held state on failure.
on_exit_hold = (ExitBySignal == True) || (ExitCode != 0)

# Periodically retry the jobs every 10 minutes, up to a maximum of 5 retries.
periodic_release =  (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 600)

# remove job if it fails 5 times.
periodic_remove = (NumJobStarts == 5)

transfer_input_files = intermediate

#stream_output = True
output = part2.stdout.\$(Process).log
error = part2.stderr.\$(Process).log
log = part2.condor.out

transfer_output_files = rep02
arguments = $NFIB 02
queue

transfer_output_files = rep03
arguments = $NFIB 03
queue

transfer_output_files = rep04
arguments = $NFIB 04
queue

transfer_output_files = rep05
arguments = $NFIB 05
queue

transfer_output_files = rep06
arguments = $NFIB 06
queue

transfer_output_files = rep07
arguments = $NFIB 07
queue

transfer_output_files = rep08
arguments = $NFIB 08
queue

transfer_output_files = rep09
arguments = $NFIB 09
queue

transfer_output_files = rep10
arguments = $NFIB 10
queue

EOF

cat > part3.submit << EOF
universe = vanilla
executable = network_jobs.sh

$project
Requirements = HAS_SINGULARITY == TRUE
+SingularityImage = "/cvmfs/singularity.opensciencegrid.org/brainlife/mcr:neurodebian1604-r2017a"
+SingularityBindCVMFS = True

should_transfer_files = IF_NEEDED
when_to_transfer_output = ON_EXIT

# Send the job to Held state on failure.
on_exit_hold = (ExitBySignal == True) || (ExitCode != 0)

# Periodically retry the jobs every 10 minutes, up to a maximum of 5 retries.
periodic_release =  (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 600)

# remove job if it fails 5 times.
periodic_remove = (NumJobStarts == 5)

# requirements = OSGVO_OS_STRING == "RHEL 7" && Arch == "X86_64" && HAS_MODULES == True 
request_memory = 12GB

#stream_output = True
output = part3.stdout.\$(Process).log
error = part3.stderr.\$(Process).log
log = part3.condor.out

transfer_input_files = compiled/main,$NFILE,rep02/track.tck
transfer_output_files = net02
arguments = net02
queue

transfer_input_files = compiled/main,$NFILE,rep03/track.tck
transfer_output_files = net03
arguments = net03
queue

transfer_input_files = compiled/main,$NFILE,rep04/track.tck
transfer_output_files = net04
arguments = net04
queue

transfer_input_files = compiled/main,$NFILE,rep05/track.tck
transfer_output_files = net05
arguments = net05
queue

transfer_input_files = compiled/main,$NFILE,rep06/track.tck
transfer_output_files = net06
arguments = net06
queue

transfer_input_files = compiled/main,$NFILE,rep07/track.tck
transfer_output_files = net07
arguments = net07
queue

transfer_input_files = compiled/main,$NFILE,rep08/track.tck
transfer_output_files = net08
arguments = net08
queue

transfer_input_files = compiled/main,$NFILE,rep09/track.tck
transfer_output_files = net09
arguments = net09
queue

transfer_input_files = compiled/main,$NFILE,rep10/track.tck
transfer_output_files = net10
arguments = net10
queue

EOF

cat > submit.dag << EOF
JOB  PART1  part1.submit
JOB  PART2  part2.submit
JOB  PART3  part3.submit
PARENT PART1 CHILD PART2
PARENT PART2 CHILD PART3
SCRIPT POST PART3 clean.sh 
EOF

condor_submit_dag -f submit.dag | grep "submitted to cluster" | cut -d " " -f 6 > jobid
#condor_submit_dag -terse submit.dag | cut -f 1 -d " " > jobid

